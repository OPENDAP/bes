
 $Id: TODO,v 1.5 1997/12/16 01:46:09 jimg Exp $

 * When given "http://podaac.jpl.nasa.gov/dods-bin/nph-hdf/dods/nscat/L30/
   S3097034.HDF?Avg_Wind_Vel_U[0:4:299][0:4:719]" writeval exits saying
   `Virtual memory exceeded in `new'', while replacing the 4 with a 1 in the
   stride works. This appears to be due to an error in the hdf server; with a
   stride of 4 it returns a very large size 5 million+.
   [open 4/7/97 george]
   [closed 5/6/97 todd]

Bad dataset names, once received by the dds filter (and others?), cause the
filters to create an `empty file' (it has a size of 2503 bytes) in addition
to the *.das and *.dds cache files it creates.

Cache files are very sensitive to access rights and directory/file ownership -
That's OK, but there should be a good discussion of how the directory needs
to be set up in the README. 

The DAS and DDS cache code seems to write new cached files for each access -
that is the last-modified data of the files tracks the last access time (on
Dec Alpha OSF 1/3.0). If might be that the files are opened for rw access.
However, see the next item...

If an installer checks the filters by running one (without the dispatch
script) then cache files are created as owned by that user. A subsequent
network access will fail because the `user' becomes nobody - or whatever the
web server has been configured as - and cannot write to those cache files (so
this problem is in part a function of the cache files apparently being
written to on every access).

Cached DAS/DDS objects should not use the .das/.dds extensions since those
are reserved for ancillary DAS/DDS objects.
[open 8/5/97 jhrg]

The Grid data type is not sent correctly when constrained - only the array
part of the grid is sent.
[open 9/9/97 jhrg]

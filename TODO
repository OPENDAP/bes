
 $Id: TODO,v 1.1 1997/05/01 23:04:54 jimg Exp $

Bad dataset names, once received by the dds filter (and others?), cause the
filters to create an `empty file' (it has a size of 2503 bytes) in addition
to the *.das and *.dds cache files it creates.

Cache files are very sensitive to access rights and directory/file ownership -
That's OK, but there should be a good discussion of how the directory needs
to be set up in the README. 

The DAS and DDS cache code seems to write new cached files for each access -
that is the last-modified data of the files tracks the last access time (on
Dec Alpha OSF 1/3.0). If might be that the files are opened for rw access.
However, see the next item...

If an installer checks the filters by running one (without the dispatch
script) then cache files are created as owned by that user. A subsequent
network access will fail because the `user' becomes nobody - or whatever the
web server has been configured as - and cannot write to those cache files (so
this problem is in part a function of the cache files apparently being
written to on every access).

When given "http://podaac.jpl.nasa.gov/dods-bin/nph-hdf/dods/nscat/L30/
S3097034.HDF?Avg_Wind_Vel_U[0:4:299][0:4:719]" writeval exits saying `Virtual
memory exceeded in `new'', while replacing the 4 with a 1 in the stride
works. This appears to be due to an error in the hdf server; with a stride of
4 it returns a very large size 5 million+.
[open 4/7/97 george]

